---
title: "Engagement Project Technical Supplement Analyses and Figures"
author: "Sarah Gripshover"
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  pdf_document:
    fig_width: 9
    fig_height: 7
    fig_caption: yes
    toc: yes
    toc_depth: 2
  html_document:
    fig_caption: yes
    fig_height: 7
    fig_width: 9
    toc_depth: 2  
---

This document contains all supplementary and exploratory analyses that were used to write the Study 1 section of the [technical supplement to the EP report](https://docs.google.com/document/d/1RJsKdoXp9uonxbs82f2mJvT5eiFkRV2IkISeGjPlBJs/edit?pli=1#). It is based on Dan Greene's cleaning and analysis document entitled "4 - EP analysis 18-19.Rmd". Dan's file was used to obtain all _preregistered_ results that appear in the technical supplement.

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      results = 'asis',
                      fig.width = 9,
                      fig.height = 7)

```

```{r, include=FALSE}

############### USER-DEFINED INPUTS #####################################

general_data_folder_crypt_path <- "Data for 2018-2019 EP analysis"
input_data_crypt_path <- "Data for 2018-2019 EP analysis/4 - 2018-2019 cleaned data for PERTS analysis v1.2.csv"

```

```{r, results='hide', echo = FALSE}

############### LOAD LIBRARIES AND PATHS #####################################

options(stringsAsFactors = FALSE)
options(xtable.comment = FALSE)
github_base_path <- "https://raw.githubusercontent.com/PERTS/gymnast/master/"

tryCatch({
    source("~/Sites/gymnast/R/util.R")
    gymnast_install()
    library(tidyverse)
    source("~/Sites/gymnast/R/util_data_summaries.R")
    source("~/Sites/gymnast/R/util_qualtrics_cleaning.R")
    source("~/Sites/gymnast/R/util_graphing.R")
    source("~/Sites/gymnast/R/util_scale_computation.R")
    source("~/Sites/gymnast/R/util_dfc.R")
}, error = function(e){
    source(paste0(github_base_path,"R/util.R")) # Need to load %+% function if missing
    gymnast_install()
    source(github_base_path %+% "R/util_data_summaries.R")
    source(github_base_path %+% "R/util_qualtrics_cleaning.R")
    source(github_base_path %+% "R/util_graphing.R")
    source(github_base_path %+% "R/util_scale_computation.R")
    source(github_base_path %+% "R/util_dfc.R")
})

# library(Rmisc) # For multiplot
library(tidyverse)
library(lubridate)
library(kableExtra)
library(scales)

# Set "today" - by default, use the real today
TODAY <- lubridate::ymd(Sys.Date())

```

```{r}

############### LOAD DATA #####################################

d <- util.find_crypt_paths(list(a = input_data_crypt_path))$a %>%
  read.csv()



# shorten race code for presentation purposes
d$race[d$race %in% "Native American, Other, Black or African American, Hispanic or Latino"] <-
  "NatAm/Other/Black/Hisp"

# also get first-measurements
d_first <- dplyr::filter(d, survey_ordinal %in% 1)

# Create first-to-last df for H2
# And drop all rows where survey_ordinal_ftl == 0, because they are students with just one observation.
# note that this maneuver actually doesn't work for the items that weren't administered every week.


# the main wide dataset is designed to do analyses of first and last observations
# we want each student's FIRST and LAST observation within the FIRST class in which they
# took the survey

d_long_base <- d %>%
    # identify the FIRST class in which students took the survey
    group_by(student_id) %>%
    mutate(
        students_first_surveyed_class = class_id[survey_ordinal == 1],
        # also save the number of classes and teachers for posterity.
        n_classes = n_distinct(class_id),
        n_teachers = n_distinct(teacher_id),
        n_teams = n_distinct(team_id)
    ) %>%
    dplyr::filter(class_id == students_first_surveyed_class)

# make sure the original number of unique students is preserved
if(!length(unique(d_long_base$student_id)) == length(unique(d$student_id))){
    stop("filtering to each student's first-surveyed class resulted in students being added or dropped from the dataset.
         Investigate before proceeding.")
} else{
    util.passed("filtering to each student's first-surveyed class preserved the original number of students. Nice!")
}

# and make sure that each student now only has ONE unique class represented in the dataset:

nrows_test <- d_long_base %>%
    group_by(student_id) %>%
    summarise(unique_classes = n_distinct(class_id)) %>%
    group_by(unique_classes) %>%
    summarise(n = n()) %>%
    nrow

if(length(nrows_test) != 1){
    stop("d_long_base has students with more than one class_id, when one is expected (or could be zero as well but that's less likely). Investigate further before proceeding.")
} else{
    util.passed("all students in d_long_base have exactly one class_id. Nice!")
}

# some helper functions for creating the wide dataset:
# for those we have to do something else.

last_of_more_than_one <- function(x){
	# takes ordered vector x and returns NA if x has one non-blank element, otherwise returns the last element
    if(length(x) < 2){
        return(NA)
    } else(
        return(x[length(x)])
    )
}
# quick tests:
# last_of_more_than_one(c(1,2,3,4,5)) # 5
# last_of_more_than_one(c(1,2,3,NA,5)) # 5
# last_of_more_than_one(5) # NA

take_nonblanks <- function(x){
    # returns all non-blank elements of x
    return(x[!util.is_blank(x)])
}

#take_nonblanks(c("", 1, "a", NA))
# [1] "1" "a"

# now take the wide datset. Note that if this EVER has to be done again, it should probably be done by
# using mutate to mark first and last observations for each student/variable combo, then casting. But whatev.
# note that adding course_id, teacher_id, and team_id as grouping vars should be no different from just
# grouping on student, because d_long_base already filters out multiple classes
d_ftl <- d_long_base %>%    
    group_by(student_id, class_id, teacher_id, team_id) %>%
    summarise(
        # first and last values
                
        # Note that we always take the first non-blank value for "first" observations, and for "last" 
        # observations we take the last non-blank value UNLESS a subject has only one value, and in that case NA is returned.
        # note that occasionally this will mean a person's "first" and "last" values can be from different observations
        # for different variables. Use the last_*_ordinal and days_between variables to make sense of these differences
        # in cadence. We've decided that this is ok.
            
        # engagement
        effort_first = first(take_nonblanks(effort)),
        effort_last = last_of_more_than_one(take_nonblanks(effort)),
        attn_first = first(take_nonblanks(attn)),
        attn_last = last_of_more_than_one(take_nonblanks(attn)),
        
        #combined lc score
        comb_lc_score_first = first(take_nonblanks(comb_lc_score)),
        comb_lc_score_last = last_of_more_than_one(take_nonblanks(comb_lc_score)),
        
        # subscales
        fg_first = first(take_nonblanks(fg)),
        fg_last = last_of_more_than_one(take_nonblanks(fg)),
        mw_first = first(take_nonblanks(mw)),
        mw_last = last_of_more_than_one(take_nonblanks(fg)),
        tc_first = first(take_nonblanks(tc)),
        tc_last = last_of_more_than_one(take_nonblanks(fg)),
        
        #items
        tc1_first = first(take_nonblanks(tc1_2)),
        tc1_last = last_of_more_than_one(take_nonblanks(tc1_2)),
        tc1_good_first = tc1_first >= 6,
        tc1_good_last = tc1_last >= 6,
        tc2_first = first(take_nonblanks(tc2_2)),
        tc2_last = last_of_more_than_one(take_nonblanks(tc2_2)),
        tc2_good_first = tc2_first >= 6,
        tc2_good_last = tc2_last >= 6,
        tc4_first = first(take_nonblanks(tc4_2)),
        tc4_last = last_of_more_than_one(take_nonblanks(tc4_2)),
        tc4_good_first = tc4_first >= 6,
        tc4_good_last = tc4_last >= 6,
        mw1_first = first(take_nonblanks(mw1_2)),
        mw1_last = last_of_more_than_one(take_nonblanks(mw1_2)),
        mw1_good_first = mw1_first >= 6,
        mw1_good_last = mw1_last >= 6,
        mw2_first = first(take_nonblanks(mw2_2)),
        mw2_last = last_of_more_than_one(take_nonblanks(mw2_2)),
        mw2_good_first = mw2_first >= 6,
        mw2_good_last = mw2_last >= 6,
        mw3_first = first(take_nonblanks(mw2_2)),
        mw3_last = last_of_more_than_one(take_nonblanks(mw3_2)),
        mw3_good_first = mw3_first >= 6,
        mw3_good_last = mw3_last >= 6,
        fg1_first = first(take_nonblanks(fg1_2)),
        fg1_last = last_of_more_than_one(take_nonblanks(fg1_2)),
        fg1_good_first = fg1_first >= 6,
        fg1_good_last = fg1_last >= 6,
        fg2_first = first(take_nonblanks(fg2_2)),
        fg2_last = last_of_more_than_one(take_nonblanks(fg2_2)),
        fg2_good_first = fg2_first >= 6,
        fg2_good_last = fg2_last >= 6,
        fg3_first = first(take_nonblanks(fg3_2)),
        fg3_last = last_of_more_than_one(take_nonblanks(fg3_2)),
        fg3_good_first = fg3_first >= 6,
        fg3_good_last = fg3_last >= 6,
        
        # validation items
        expected_grade_first = first(take_nonblanks(expected_grade)),
        expected_grade_last = last_of_more_than_one(take_nonblanks(expected_grade)),
        gms_first = first(take_nonblanks(gms)),
        gms_last = last_of_more_than_one(take_nonblanks(gms)),
        belonging_first = first(take_nonblanks(belonging)),
        belonging_last = last_of_more_than_one(take_nonblanks(belonging)),
        teacher_mastery_first = first(take_nonblanks(teacher_mastery)),
        teacher_mastery_last = last_of_more_than_one(take_nonblanks(teacher_mastery)),
        
        # fidelity
        tuq_high_first = first(take_nonblanks(teacher_use_qs >= 4)),
        tuq_high_last = last_of_more_than_one(take_nonblanks(teacher_use_qs >= 4)),
        
        # demographics
        gender = first(gender),
        race = first(race),
        prior_gpa = first(prior_gpa),
        
        n_classes = first(n_classes),
        n_teachers = first(n_teachers),
        n_teams = first(n_teams),
        
        # timing variables
        
        # last LC survey ordinal is the last time they got a learning conditions survey
        # so it's the survey_ordinal corresponding to the last non-blank lc value
        # these are useful for summarizing information about how the first and last timepoints relate to each other
        last_lc_survey_ordinal = last_of_more_than_one(survey_ordinal[!util.is_blank(comb_lc_score)]),
        last_gms_survey_ordinal = last_of_more_than_one(survey_ordinal[!util.is_blank(gms)]),
        last_belonging_survey_ordinal = last_of_more_than_one(survey_ordinal[!util.is_blank(belonging)]),
        last_teacher_mastery_survey_ordinal = last_of_more_than_one(survey_ordinal[!util.is_blank(teacher_mastery)]),
        last_effort_survey_ordinal = last_of_more_than_one(survey_ordinal[!util.is_blank(effort)]),
        
        # also get a sense of when the surveys were taken to have some understanding of seasonality, delay etc.
        first_lc_date = as.Date(first(StartDate[!util.is_blank(comb_lc_score)])),
        last_lc_date = as.Date(last_of_more_than_one(StartDate[!util.is_blank(comb_lc_score)])),
        first_gms_date = as.Date(first(StartDate[!util.is_blank(gms)])),
        last_gms_date = as.Date(last_of_more_than_one(StartDate[!util.is_blank(gms)])),
        
        # days_between first and last observations
        days_between_lc_ftl = difftime(last_lc_date, first_lc_date, units = "days"),
        days_between_gms_ftl = difftime(last_gms_date, first_gms_date, units = "days")

    ) %>%
    mutate(
        # secondary column calculations (based on columns computed in the summarise() call)
        comb_lc_score_first_bins = floor(comb_lc_score_first),
        comb_lc_score_last_bins = floor(comb_lc_score_last),
        
         # cutoffs
        gms_str_agr_first = ifelse(gms_first == 5, 1, 0),
        gms_str_agr_last = ifelse(gms_last == 5, 1, 0),
        belonging_str_agr_first = ifelse(belonging_first == 5, 1, 0),
        belonging_str_agr_last = ifelse(belonging_last == 5, 1, 0),
        
        gms_agr_first = ifelse(gms_first >= 4, 1, 0),
        gms_agr_last = ifelse(gms_last >= 4, 1, 0),
        belonging_agr_first = ifelse(belonging_first >= 4, 1, 0),
        belonging_agr_last = ifelse(belonging_last >= 4, 1, 0),
        
        # deltas
        effort_ftl = effort_last - effort_first,
        attn_ftl = attn_last - attn_first,
        comb_lc_score_ftl = comb_lc_score_last - comb_lc_score_first,
        fg_ftl = fg_last - fg_first,
        mw_ftl = mw_last - mw_first,
        tc_ftl = tc_last - tc_first,
        expected_grade_ftl = expected_grade_last - expected_grade_first,
        gms_ftl = gms_last - gms_first,
        belonging_ftl = belonging_last - belonging_first,
        teacher_mastery_ftl = teacher_mastery_last - teacher_mastery_first,
        gms_str_agr_first_ftl = gms_str_agr_last - gms_str_agr_first,
        
        # improvement 
        lc_improved = comb_lc_score_ftl > 0

    )

# check that d_ftl is indeed wide
if(!nrow(d_ftl) == length(unique(d_ftl$student_id))){
    stop("d_ftl is supposed to be wide by student_id, but it's not. Investigate futher.")
} else{
    util.passed("d_ftl is wide by student_id. Nice!")
}

# how many students had more than one class, teacher, etc.
# d_ftl %>%
#     group_by(mult_classes = n_classes > 1, mult_teachers = n_teachers > 1, mult_teams = n_teams > 1) %>%
#     summarise(
#         n = n(),
#         prop = n/nrow(d_ftl)
#     )

# but in some cases we may want to consider ordinal survey values in the wide data, like when computing psych scales
# or maybe someday doing cross-lagged analyses

individual_items <- c("tc1_2", "tc2_2", "tc4_2", "fg1_2", "fg2_2", "fg3_2", "mw1_2", "mw2_2", "mw3_2",
                     "effort", "attn", "gms_2.0", "gms_3.0", "belong_4", "belong_3.1", "mastery1", "mastery2")

d_wide_casted <- d[c("student_id", "survey_ordinal", individual_items)] %>%
    melt(., id.vars = c("student_id", "survey_ordinal")) %>%
    separate(variable, into = c("variable", "version"), sep = "_" ) %>%
    # the separation breaks the gms and belonging variable notation so fix them by hand
    mutate(
        variable = ifelse(variable %in% c("gms", "belong"), variable %+% version, variable)
    ) %>%
    dcast(student_id ~ variable + survey_ordinal) %>%
    # now filter out really ridiculous numbers of observations generating columns filled with mostly blank data
    # we'll keep everyone's data only through their NINTH observation, which eliminates about 5% of the data
    # table(d$survey_ordinal > 9)/nrow(d). Note that LAST observations will still be accurate bc these were pulled from the wide data
    select(-matches("_[1-9][0-9]"))

# and here's a fidelity df based on Dan Greene's original code for determining high fidelity classes in
# the script 4 EP analysis 18-19.Rmd
# ...crap. I think we actually can't use the wide data analyses. Because the items are only supposed to be valid
# WITHIN classes. Students should be able to give different answers within classes.

# option 1: repeat all analyses using ONLY students who were in only one class
# option 2: repeat all analyses using data from all students, and fitting intercepts for students (and all that jazz)
# option 3: keep the analyses we have

# ... ok MAYBE what I could do is consider each student's first CLASS only. How would that work?

# I think I'm going to go ahead and finish what I have to do, but then deal with this nesting BS
# but I can't really do it without cycles, or ordinal class surveys, can I?
# I could take all students' first and last observations within classes, I suppose

# Make a data.frame that summarises to the class level (1 row per class)
# get first-to-last scores for each student WITHIN classes (as opposed to the wide dataset which considers only students)

# now merge them all together

d_wide <- merge(d_ftl, d_wide_casted)

# quickly check the merges
if(!nrow(d_wide) == nrow(d_wide_casted) | !nrow(d_wide) == nrow(d_ftl)){
    stop("inner joins d_ftl, d_wide_casted, and fidelity_df resulted in rows being added or dropped. These objects should share a primary key with perfectly overlapping values (i.e., the column student_id). Investigate further before proceeding.")
} else{
    util.passed("Joins to form wide df maintained expected number of rows. Nice!")
}

```
# Demograhics
```{r}
d_wide %>%
    group_by(race) %>%
    summarise(n = n()) %>%
    mutate(race = ifelse(is.na(race), "Missing", race)) %>%
    mutate(race = ifelse(race %in% "masked", "Masked", race)) %>%
    xtable()

d_wide %>%
    group_by(gender) %>%
    summarise(n = n()) %>%
    mutate(gender = ifelse(is.na(gender), "Missing", gender)) %>%
    mutate(gender = ifelse(gender %in% "masked", "Masked", gender)) %>%
    xtable()

```
# Psychometrics

```{r}

# make the scale key by hand.Note that the word "scale" in the column name is used to pick out columns indicating scale membership.
# so be sure to keep this naming convention if you edit the doc. (Don't name any non-scale columns using the word "scale" and don't 
# leave the word "scale" out of scale columns.)

# scale_vars <- data.frame(
#     var_name = names(d_wide)[grepl("_1", names(d_wide))],
#     scale = c("attention", "belonging", "belonging", "effort", "lc_combined", "lc_combined", "lc_combined", "gms", "gms", "teacher_mastery", "teacher_mastery", "lc_combined", "lc_combined", "lc_combined", "lc_combined", "lc_combined", "lc_combined")
# )

# now create a set of scales that drops the item that was missing from study 2
# scale_vars$scale_drop1 <- scale_vars$scale
# scale_vars$scale_drop1[scale_vars$scale %in% "lc_combined"] <- "lc_drop_one"
# scale_vars$scale_drop1[scale_vars$var_name %in% "fg1_1"] <- NA
# 
# # and a set of scales for individual LCs
# scale_vars$scale_indiv_lcs <- scale_vars$scale
# scale_vars$scale_indiv_lcs[grepl("^fg", scale_vars$var_name)] <- "fg"
# scale_vars$scale_indiv_lcs[grepl("^tc", scale_vars$var_name)] <- "tc"
# scale_vars$scale_indiv_lcs[grepl("^mw", scale_vars$var_name)] <- "mw"

# # write to csv so that future versions can be hand-edited
# write.csv(scale_vars, "~/Sites/analysis/Engagement Project 2018-19/2018-2019 EP analysis/scale_variables_all_lc_combined.csv", row.names = FALSE)

# now read it in
scale_vars <- read.csv("~/Sites/analysis/Engagement Project 2018-19/2018-2019 EP analysis/scale_variables_all_lc_combined.csv", stringsAsFactors = FALSE)

# find the unique values in all the scales columns.
scales <- scale_vars %>%
    select(matches("scale")) %>%
    unlist %>%
    unname %>%
    unique
scales <- scales[!is.na(scales)]


alphas <- data.frame(
    scale = scales,
    alpha = NA
)

# for each unique scale name, I want to find the items associated with it, and compute an alpha for those items.

for(s in scales){
    # find all the columns in which a scale label appears    
    scale_type_cols <- names(scale_vars)[unlist(util.apply_columns(scale_vars, function(x) s %in% x ))]
   
   
   # if more than one, check that each column always picks out the same items
    if(length(scale_type_cols) == 1){
        items <- scale_vars$var_name[scale_vars[, scale_type_cols] %in% s]
    } else if(length(scale_type_cols) > 1){
        # get a list of the items specified by each scale label
        item_row_indexes <- lapply(scale_vars[scale_type_cols], function(x) which(x %in% s))
        items <- lapply(item_row_indexes, function(x) return(scale_vars$var_name[x]))
        
        # check that each column picked out the same items
        if(!all(sapply(items, function(x) all(x == items[[1]])))){
            stop("Your scale_vars df assigns the same scale label to different groups of items across columns. You have to fix this in order to proceed.")
        }
        
        items <- items %>% unlist %>% unique
        
    } else if(length(scale_type_cols) == 0){
        util.warn("for scale " %+% s %+% ", no matches were found in scale_vars data.frame. Moving on to next variable")
        items <- NA
        next
    }
    if(length(items) > 1){
        alphas$alpha[alphas$scale %in% s] <- psych::alpha(d_wide[items])$total$raw_alpha
    } else{
        alphas$alpha[alphas$scale %in% s] <- NA
    }
    
}



xtable(alphas)




# if not, throw a warning and move on

# if so, find the items and put them in the data.frame


# then go through the list and pull out

```

# Improved learning conditions

The Brief Report states that 70% of teachers improved one or more learning conditions when they implemented the Engagement Project (with fidelity).

```{r}

# this requires a teacher-level dataset, maybe a class one too. sset cutsom function to pull 
# aggregated metrics
pull_summary_metrics <- function(grouped_df, suffix = ""){
    # takes a grouped_df and returns a df summarized to the level of the grouping vars
    # with all the specific metrics calculatd. Suffixes can be appended to the summarised cols
    # for easier merging
    
    mean_narm <- function(x) mean(x, na.rm = T)
    summary_df <- grouped_df %>%
        summarise(
            n = n(),
            attn_first = mean_narm(effort_first),
            attn_last = mean_narm(effort_last),
            effort_first = mean_narm(effort_first),
            effort_last = mean_narm(effort_last),
            expected_grade_first = mean_narm(expected_grade_first),
            expected_grade_last = mean_narm(expected_grade_last),
            comb_lc_score_first = mean_narm(comb_lc_score_first),
            comb_lc_score_last = mean_narm(comb_lc_score_last),
            tc_pct_good_first = mean_narm(tc_first >= 6),
            tc_pct_good_last = mean_narm(tc_last >= 6),
            fg_pct_good_first = mean_narm(fg_first >= 6),
            fg_pct_good_last = mean_narm(fg_last >= 6),
            mw_pct_good_first = mean_narm(mw_first >= 6),
            mw_pct_good_last = mean_narm(mw_last >= 6),
            tc_first = mean_narm(tc_first),
            tc_last = mean_narm(tc_last),
            fg_first = mean_narm(fg_first),
            fg_last = mean_narm(fg_last),
            mw_first = mean_narm(mw_first),
            mw_last = mean_narm(mw_last),
            gms_first = mean_narm(gms_first),
            gms_last = mean_narm(gms_last),
            belonging_first = mean_narm(belonging_first),
            belonging_last = mean_narm(belonging_last),
            teacher_mastery_first = mean_narm(teacher_mastery_first),
            teacher_mastery_last = mean_narm(teacher_mastery_last),
            pct_tuq_high_first = mean(tuq_high_first, na.rm = TRUE),
            pct_tuq_high_last = mean(tuq_high_last, na.rm = TRUE)
        ) %>%
        mutate(
            effort_ftl = effort_last - effort_first,
            attn_ftl = attn_last - attn_first,
            comb_lc_score_ftl = comb_lc_score_last - comb_lc_score_first,
            fg_ftl = fg_last - fg_first,
            mw_ftl = mw_last - mw_first,
            tc_ftl = tc_last - tc_first,
            fg_pct_good_ftl = fg_pct_good_last - fg_pct_good_first,
            mw_pct_good_ftl = mw_pct_good_last - mw_pct_good_first,
            tc_pct_good_ftl = tc_pct_good_last - tc_pct_good_first,
            expected_grade_ftl = expected_grade_last - expected_grade_first,
            gms_ftl = gms_last - gms_first,
            belonging_ftl = belonging_last - belonging_first,
            teacher_mastery_ftl = teacher_mastery_last - teacher_mastery_first,
            tuq_over_80_first = pct_tuq_high_first >= .8,
            tuq_over_80_last = pct_tuq_high_last >= .8,
            fidelity_first = ifelse(tuq_over_80_first, "high fidelity", ifelse(!tuq_over_80_first, "low fidelity", NA)),
            fidelity_last = ifelse(tuq_over_80_last, "high fidelity", ifelse(!tuq_over_80_last, "low fidelity", NA))
        ) %>%
        util.apply_columns(., function(x) ifelse(is.nan(x), NA, x))
        # get rid of annoying non-finite values
    # now add the suffix to everything but the grouping vars
    old_names <- names(summary_df)
    new_names <- ifelse(old_names %in% group_vars(grouped_df), old_names, paste0(old_names, suffix))
    names(summary_df) <- new_names
    return(summary_df)
}

# wait...do I want to define fidelity at the class level?
# note — groupings assume classes nested within teachers within teams. Assumption holds for 2018-19 but
# could be otherwise in the future

d_teacher <- d_wide %>%
    group_by(teacher_id, team_id) %>%
    pull_summary_metrics(., "_teacher")

d_class <- d_wide %>%
    group_by(class_id, teacher_id, team_id) %>%
    pull_summary_metrics(., "_class")

d_teacher %>%
   # mutate(tuq_over_80_last_teacher = ifelse(is.na(tuq_over_80_last_teacher), "no last obs", tuq_over_80_last_teacher)) %>%
    #group_by(tuq_over_80_last_teacher) %>%
    dplyr::filter(n_teacher >= 10) %>%
    summarise(
        n = sum(!is.na(tc_ftl_teacher) | !is.na(fg_ftl_teacher) | !is.na(mw_ftl_teacher)),
        improved_one_lc_prop = mean(tc_ftl_teacher > 0 | fg_ftl_teacher > 0 | mw_ftl_teacher > 0, na.rm = T),
        improved_all_lcs_prop = mean(tc_ftl_teacher > 0 & fg_ftl_teacher > 0 & mw_ftl_teacher > 0, na.rm = T),
        improved_one_lc_total = sum(tc_ftl_teacher > 0 | fg_ftl_teacher > 0 | mw_ftl_teacher > 0, na.rm = T),
        improved_all_lcs_total = sum(tc_ftl_teacher > 0 & fg_ftl_teacher > 0 & mw_ftl_teacher > 0, na.rm = T),
        improved_tc_prop = mean(tc_ftl_teacher > 0, na.rm = T),
        improved_fg_prop = mean(fg_ftl_teacher > 0, na.rm = T),
        improved_mw_prop = mean(mw_ftl_teacher > 0, na.rm = T),
        improved_tc_total = sum(tc_ftl_teacher > 0, na.rm = T),
        improved_fg_total = sum(fg_ftl_teacher > 0, na.rm = T),
        improved_mw_total = sum(mw_ftl_teacher > 0, na.rm = T)
        
    ) %>%
    util.apply_columns(., function(x) ifelse(is.nan(x), NA, x)) %>%
    xtable()


d_class %>%
   # mutate(tuq_over_80_last_class = ifelse(is.na(tuq_over_80_last_class), "no last obs", tuq_over_80_last_class)) %>%
    group_by(tuq_over_80_last_class) %>%
    dplyr::filter(n_class >= 10) %>%
    summarise(
        n = sum(!is.na(tc_ftl_class) | !is.na(fg_ftl_class) | !is.na(mw_ftl_class)),
        improved_one_lc_prop = mean(tc_ftl_class > 0 | fg_ftl_class > 0 | mw_ftl_class > 0, na.rm = T),
        improved_all_lcs_prop = mean(tc_ftl_class > 0 & fg_ftl_class > 0 & mw_ftl_class > 0, na.rm = T),
        improved_one_pct_good_prop = mean(tc_pct_good_ftl_class > 0 | fg_pct_good_ftl_class > 0 | mw_pct_good_ftl_class > 0, na.rm = T),
        improved_one_lc_total = sum(tc_ftl_class > 0 | fg_ftl_class > 0 | mw_ftl_class > 0, na.rm = T),
        improved_all_lcs_total = sum(tc_ftl_class > 0 & fg_ftl_class > 0 & mw_ftl_class > 0, na.rm = T),
        improved_tc_prop = mean(tc_ftl_class > 0, na.rm = T),
        improved_fg_prop = mean(fg_ftl_class > 0, na.rm = T),
        improved_mw_prop = mean(mw_ftl_class > 0, na.rm = T),
        improved_tc_total = sum(tc_ftl_class > 0, na.rm = T),
        improved_fg_total = sum(fg_ftl_class > 0, na.rm = T),
        improved_mw_total = sum(mw_ftl_class > 0, na.rm = T)
        
    ) %>%
    util.apply_columns(., function(x) ifelse(is.nan(x), NA, x)) %>%
    xtable()

# why the discrepancy with the dashboard results?
length(unique(d$class_id)) # 251 classes in d
length(unique(d_long_base$class_id)) # 245 — so 6 classes were eliminated by taking first observated classes for individ students
length(unique(d_wide$class_id)) # 245 classes in d_wide
length(unique(d_class$class_id)) # 245 so, same
length(unique(d_class$class_id[!is.na(d_class$effort_ftl_class)])) # 150. So a lot of classes only surveyed once

# no effing clue why they're so different. But they are. I need to investigate further but my brain hurts rn

t_test_result <- t.test(d_teacher$comb_lc_score_ftl_teacher, mu = 0)

# improvement for each individual LC?
t_test_result_fg <- t.test(d_teacher$fg_ftl_teacher, mu = 0) # yes
t_test_result_mw <- t.test(d_teacher$mw_ftl_teacher, mu = 0) # yes
t_test_result_tc <- t.test(d_teacher$tc_ftl_teacher, mu = 0) # no! there's a decrease!

t.test(d_class$comb_lc_score_ftl_class, mu = 0)
t.test(d_class$fg_ftl_class[d_class$fidelity_first_class %in% "high fidelity"], mu = 0)
t.test(d_class$mw_ftl_class[d_class$fidelity_first_class %in% "high fidelity"], mu = 0)
t.test(d_class$tc_ftl_class[d_class$fidelity_first_class %in% "high fidelity"], mu = 0)

# How to describe in report
## Which has largest increase by teacher?
d_teacher_desc <- d_teacher %>%
  gather(max_lc_desc, max_lc_value, c(fg_ftl_teacher,mw_ftl_teacher,tc_ftl_teacher)) %>% 
  group_by(teacher_id) %>% 
  slice(which.max(max_lc_value)) 

table(d_teacher_desc$max_lc_desc[d_teacher_desc$max_lc_value>0])
# 54/61 teacher had greatest improvements in Meaningful Work

# Model of students' change in mw
mw_change_mod <- lmer(mw_ftl ~ gender + race + prior_gpa + ( 1 | team_id ),
             data = d_ftl)
summary(mw_change_mod)

# How many students above midpoint at pre and post?
with(d_ftl[!is.na(d_ftl$mw_ftl),],table(mw_first >=4)) # 1352 / 2514 = 54%
with(d_ftl[!is.na(d_ftl$mw_ftl),],table(mw_last >=4)) # 1942 / 2514 = 77%

with(d_ftl[!is.na(d_ftl$fg_ftl),],table(tc_first >=4))
with(d_ftl[!is.na(d_ftl$fg_ftl),],table(tc_last <=4))



# Eric looking for improvement discrepancy
## ftl = first_to_last
## 102 teacher with 245 classes
table(d_class$fidelity_first_class,d_class$fidelity_last_class) # 169 high fidelity by first_class; #101 high fidelity by first and last class

# If we look at each class
hist(d_class$fg_ftl_class)
hist(d_class$mw_ftl_class)
hist(d_class$tc_ftl_class)

# Let's go back to student
hist(d_ftl$fg_last - d_ftl$fg_first) # Good - individual student movement
t.test(d_ftl$fg_last - d_ftl$fg_first) # .20
hist(d_ftl$mw_last - d_ftl$mw_first) # Good - individual student movement
t.test(d_ftl$mw_last - d_ftl$mw_first) # .60
hist(d_ftl$tc_last - d_ftl$tc_first) # Reverse direction
t.test(d_ftl$tc_last - d_ftl$tc_first) # -.60

# Full sample - how many students impove on one based on fidelity

table(d_ftl$fg_last - d_ftl$fg_first > 0)/ nrow(d_ftl[!is.na(d_ftl$fg_last),]) # 50%
table(d_ftl$mw_last - d_ftl$mw_first > 0)/ nrow(d_ftl[!is.na(d_ftl$mw_last),]) # 63%
table(d_ftl$tc_last - d_ftl$tc_first > 0)/ nrow(d_ftl[!is.na(d_ftl$tc_last),]) # 21%

table(
  d_ftl$fg_last - d_ftl$fg_first > 0 |
  d_ftl$mw_last - d_ftl$mw_first > 0 | 
  d_ftl$tc_last - d_ftl$tc_first > 0
) / nrow(d_ftl[!is.na(d_ftl$fg_last) & !is.na(d_ftl$mw_last) & !is.na(d_ftl$tc_last),]) #74%

# Look at only high fidelity classes
high_fid_classes <- unique(d_class$class_id[d_class$fidelity_last_class == "high fidelity" & !is.na(d_class$fidelity_last_class)]) #118 high fidelity classes

table(ifelse(d_ftl$class_id %in% high_fid_classes, "High Fidelity","Low Fidelity"),
  d_ftl$fg_last - d_ftl$fg_first > 0 |
  d_ftl$mw_last - d_ftl$mw_first > 0 | 
  d_ftl$tc_last - d_ftl$tc_first > 0
)  # 1630 / (1630 + 549) = 75%

# What about moving from bad -> good?
table(ifelse(d_ftl$class_id %in% high_fid_classes, "High Fidelity","Low Fidelity"),
  d_ftl$fg_last - d_ftl$fg_first > 0 |
  d_ftl$mw_last - d_ftl$mw_first > 0 | 
  d_ftl$tc_last - d_ftl$tc_first > 0
)  # 1630 / (1630 + 549) = 75%

# How about for high-fidelity teachers?
table(
  d_teacher$fg_last_teacher - d_teacher$fg_first_teacher > 0 |
  d_teacher$mw_last_teacher - d_teacher$mw_first_teacher > 0 | 
  d_teacher$tc_last_teacher - d_teacher$tc_first_teacher > 0,
  useNA="always"
) 

# Classes 
table(
  d_class$fg_last_class - d_class$fg_first_class > 0 |
  d_class$mw_last_class - d_class$mw_first_class > 0 | 
  d_class$tc_last_class - d_class$tc_first_class > 0,
  useNA="always"
) 


```

## Graphs and narrative for learning conditions

```{r}
# histogram of improvement by class (combined and separate)
plot1_lc_all <- ggplot(d_class, aes(x=comb_lc_score_ftl_class)) + geom_histogram() + theme_bw() +  labs(x = "Change in Combined Learning Conditions", y = "Number of Classes") + geom_vline(aes(xintercept = mean(comb_lc_score_ftl_class,na.rm = T)),size=2, linetype= "dashed")

plot1_lc_fg <- ggplot(d_class, aes(x=fg_ftl_class)) + geom_histogram() + theme_bw() +  labs(x = "Change in Growth Feedback", y = "Number of Classes") + geom_vline(aes(xintercept = mean(fg_ftl_class,na.rm = T)),size=2, linetype= "dashed")
  
plot1_lc_mw <- ggplot(d_class, aes(x=mw_ftl_class)) + geom_histogram() + theme_bw() +  labs(x = "Change in Meaningful Work", y = "Number of Classes") + geom_vline(aes(xintercept = mean(mw_ftl_class,na.rm = T)),size=1, linetype= "dashed")

plot1_lc_tc <- ggplot(d_class, aes(x=tc_ftl_class)) + geom_histogram() + theme_bw() +  labs(x = "Change in Teacher Care", y = "Number of Classes") + geom_vline(aes(xintercept = mean(tc_ftl_class,na.rm = T)),size=2, linetype= "dashed")
  
Rmisc::multiplot(plot1_lc_all,plot1_lc_fg,plot1_lc_mw,plot1_lc_tc,cols=2)


# histogram of improvement by teacher (combined and separate)
plot2_lc_all <- ggplot(d_teacher, aes(x=comb_lc_score_ftl_teacher)) + geom_histogram() + theme_bw() +  labs(x = "Change in Combined Learning Conditions", y = "Number of Teachers") + geom_vline(aes(xintercept = mean(comb_lc_score_ftl_teacher,na.rm = T)),size=2, linetype= "dashed")

plot2_lc_fg <- ggplot(d_teacher, aes(x=fg_ftl_teacher)) + geom_histogram() + theme_bw() +  labs(x = "Change in Growth Feedback", y = "Number of Teachers") + geom_vline(aes(xintercept = mean(fg_ftl_teacher,na.rm = T)),size=2, linetype= "dashed")
  
plot2_lc_mw <- ggplot(d_teacher, aes(x=mw_ftl_teacher)) + geom_histogram() + theme_bw() +  labs(x = "Change in Meaningful Work", y = "Number of Teachers") + geom_vline(aes(xintercept = mean(mw_ftl_teacher,na.rm = T)),size=2, linetype= "dashed")

plot2_lc_tc <- ggplot(d_teacher, aes(x=tc_ftl_teacher)) + geom_histogram() + theme_bw() +  labs(x = "Change in Teacher Care", y = "Number of Teachers") + geom_vline(aes(xintercept = mean(tc_ftl_teacher,na.rm = T)),size=2, linetype= "dashed")
  
Rmisc::multiplot(plot2_lc_all,plot2_lc_fg,plot2_lc_mw,plot2_lc_tc,cols=2)

# improvement by high fidelity vs. low fidelity
  
plot3_lc_fg_high_fid <- ggplot(d_class[d_class$tuq_over_80_first_class,], aes(x=fg_ftl_class)) + geom_histogram() + theme_bw() +  labs(x = "Change in Growth Feedback\n(High Fidelity)", y = "Number of Classes") + geom_vline(aes(xintercept = mean(fg_ftl_class,na.rm = T)),size=2, linetype= "dashed") + scale_x_continuous(limits=c(-2.5,2.5))

plot3_lc_fg_low_fid <- ggplot(d_class[!d_class$tuq_over_80_first_class,], aes(x=fg_ftl_class)) + geom_histogram() + theme_bw() +  labs(x = "Change in Growth Feedback\n(Low Fidelity)", y = "Number of Classes") + geom_vline(aes(xintercept = mean(fg_ftl_class,na.rm = T)),size=2, linetype= "dashed") + scale_x_continuous(limits=c(-2.5,2.5))

plot3_lc_mw_high_fid <- ggplot(d_class[d_class$tuq_over_80_first_class,], aes(x=mw_ftl_class)) + geom_histogram() + theme_bw() +  labs(x = "Change in Meaningful Work\n(High Fidelity)", y = "Number of Classes") + geom_vline(aes(xintercept = mean(mw_ftl_class,na.rm = T)),size=2, linetype= "dashed") + scale_x_continuous(limits=c(-2.5,2.5))

plot3_lc_mw_low_fid <- ggplot(d_class[!d_class$tuq_over_80_first_class,], aes(x=mw_ftl_class)) + geom_histogram() + theme_bw() +  labs(x = "Change in Meaningful Work\n(Low Fidelity)", y = "Number of Classes") + geom_vline(aes(xintercept = mean(mw_ftl_class,na.rm = T)),size=2, linetype= "dashed") + scale_x_continuous(limits=c(-2.5,2.5))

plot3_lc_tc_high_fid <- ggplot(d_class[d_class$tuq_over_80_first_class,], aes(x=tc_ftl_class)) + geom_histogram() + theme_bw() +  labs(x = "Change in Teacher Care\n(High Fidelity)", y = "Number of Classes") + geom_vline(aes(xintercept = mean(tc_ftl_class,na.rm = T)),size=2, linetype= "dashed") + scale_x_continuous(limits=c(-2.5,2.5))

plot3_lc_tc_low_fid <- ggplot(d_class[!d_class$tuq_over_80_first_class,], aes(x=tc_ftl_class)) + geom_histogram() + theme_bw() +  labs(x = "Change in Teacher Care\n(Low Fidelity)", y = "Number of Classes") + geom_vline(aes(xintercept = mean(tc_ftl_class,na.rm = T)),size=2, linetype= "dashed") + scale_x_continuous(limits=c(-2.5,2.5))

Rmisc::multiplot(plot3_lc_fg_high_fid, plot3_lc_fg_low_fid, plot3_lc_mw_high_fid, plot3_lc_mw_low_fid, plot3_lc_tc_high_fid, plot3_lc_tc_low_fid, cols=3)



# Broken down by learning condition
## Don't have this data?  


```


# Improved effort

Students who reported learning conditions as positive in a given week were X% more likely to say they put a lot of effort.

```{r}
# object "d" was used because findings are reported at the student/week level
d$lc_agr_stragr <- d$comb_lc_score >= 6
d$lc_agree_disagree <- d$comb_lc_score >= 5

d %>%
    dplyr::filter(!is.na(lc_agree_disagree)) %>%
    group_by(lc_agree_disagree) %>%
    summarise(
        strong_effort_5 = mean(effort == 5, na.rm = T),
        strong_attn_5 = mean(attn == 5, na.rm = T)
    ) %>%
    xtable()
```
``



# Increased belonging and growth mindset
The Brief Report states that, “When learning conditions improved, students were 24% more likely to increase in growth mindset, and 90% more likely to experience a stronger sense of belonging.”

## Percentages
```{r}
d_wide %>%
    group_by(lc_improved) %>%
    dplyr::filter(
        !is.na(last_lc_survey_ordinal),
        !is.na(last_gms_survey_ordinal),
        !is.na(last_belonging_survey_ordinal)
    ) %>%
    summarise(
        n = n(),
        prop_n = n/nrow(.),
        belonging_improved = mean(belonging_ftl > 0),
        gms_improved = mean(gms_ftl > 0)
    ) %>%
    xtable()

d_wide %>%
    group_by(lc_improved) %>%
    dplyr::filter(
        !is.na(last_lc_survey_ordinal),
        !is.na(last_gms_survey_ordinal),
        !is.na(last_belonging_survey_ordinal)
    ) %>%
    nrow

```

# Overall Improvement
Sarah requested "% of teachers improved the learning conditions in their class overall (>0 overall lc_composite)"

```{r}
table(d_teacher$comb_lc_score_ftl_teacher > 0) # 65%
table(d_teacher$comb_lc_score_ftl_teacher > 0,d_teacher$fidelity_last_teacher) # 71% for high fidelity
```

## Models
```{r}
# the analyses currently reported are those that don't control for baseline gms/belonging (in parallel
# with preregistered H2). However, controlling for these two things can help rule out regression to the mean
# as an explanation for the correspondence between ftl scores. So they're here but just commented out, so that
# it's easier to turn them back on.
gms_mod <- lmer(gms_ftl ~ comb_lc_score_ftl + comb_lc_score_first #+ gms_first 
        + gender + race + prior_gpa +
         (1 | class_id) + (1 | teacher_id) + (1 | team_id), data = d_wide) 
# summary(gms_mod)
# Fixed effects:
#                                Estimate Std. Error         df t value Pr(>|t|)    
# (Intercept)                    -0.05475    0.23199  960.29049  -0.236 0.813471    
# comb_lc_score_ftl               0.12095    0.03311 1119.14504   3.653 0.000271 ***
# comb_lc_score_first             0.01928    0.03472 1096.04733   0.555 0.578743    
# genderMale, Non-Binary/Other   -0.07162    0.06116 1118.38014  -1.171 0.241815    
# gendermasked                   -0.10274    0.08620   22.08894  -1.192 0.245934    
# raceNatAm/Other/Black/Hisp     -0.04384    0.06872  705.13866  -0.638 0.523716    
# prior_gpa                      -0.02148    0.03919  737.34093  -0.548 0.583788   

class(gms_mod) <- "lmerMod"    
stargazer(gms_mod)


belonging_mod <- lmer(belonging_ftl ~ comb_lc_score_ftl + comb_lc_score_first #+ belonging_first 
        + gender + race + prior_gpa +
         (1 | class_id) + (1 | teacher_id) + (1 | team_id), data = d_wide)
# summary(belonging_mod)
# Fixed effects:
#                                Estimate Std. Error         df t value Pr(>|t|)    
# (Intercept)                    -0.38387    0.16632  876.81343  -2.308   0.0212 *  
# comb_lc_score_ftl               0.21479    0.02367 1048.69031   9.074   <2e-16 ***
# comb_lc_score_first             0.05700    0.02488  897.70257   2.291   0.0222 *  
# genderMale, Non-Binary/Other    0.06584    0.04362 1117.93956   1.509   0.1315    
# gendermasked                   -0.00878    0.06528   16.80107  -0.135   0.8946    
# raceNatAm/Other/Black/Hisp     -0.05452    0.04938  674.51936  -1.104   0.2700    
# prior_gpa                       0.02741    0.02819  649.73754   0.972   0.3313 

class(belonging_mod) <- "lmerMod"
stargazer(belonging_mod)


```

# Confirmatory analyses

## H1

### H1a: Combined LC score predicts self-reported effort.

H1a was confirmed: combined LC score predicts concurrent self-reported effort, even controlling for student demographics and class/teacher/team placement.

```{r, results='asis'}

# summary(lmer(effort ~ comb_lc_score + gender + race + prior_gpa + 
#                ( 1 | student_id ) + ( 1 | class_id ) + ( 1 | teacher_id ) + ( 1 | team_id ),
#              data = d))
# Model failed to converge - removing ( 1 | class_id )


lc_concurrent_effort_mod <- lmer(effort ~ comb_lc_score + gender + race + prior_gpa + 
               ( 1 | student_id ) + ( 1 | teacher_id ) + ( 1 | team_id ),
             data = d)
# summary(lc_concurrent_effort_mod)

#Fixed effects:
#                               Estimate Std. Error t value
# (Intercept)                   1.898562   0.077531  24.488
# comb_lc_score                 0.317270   0.008522  37.232
# genderMale, Non-Binary/Other -0.163097   0.020216  -8.068
# gendermasked                 -0.066441   0.095802  -0.694
# racemasked                    0.116832   0.264120   0.442
# raceNatAm/Other/Black/Hisp    0.049501   0.022508   2.199
# prior_gpa                     0.154474   0.013842  11.160
```    

### H1b

```{r}
lc_concurrent_exp_grade_mod <- lmer(expected_grade ~ comb_lc_score + gender + race + prior_gpa + 
               ( 1 | student_id ) + ( 1 | teacher_id ) + ( 1 | team_id ),
             data = d)
# summary(lc_concurrent_exp_grade_mod)
# Fixed effects:
#                                Estimate Std. Error         df t value Pr(>|t|)    
# (Intercept)                   1.435e+00  6.328e-02  3.795e+02  22.677   <2e-16 ***
# comb_lc_score                 9.512e-02  7.847e-03  6.491e+03  12.121   <2e-16 ***
# genderMale, Non-Binary/Other  2.016e-02  1.668e-02  4.605e+03   1.208   0.2270    
# gendermasked                 -2.904e-03  6.849e-02  2.384e+01  -0.042   0.9665    
# racemasked                    5.224e-01  2.783e-01  7.901e+02   1.877   0.0609 .  
# raceNatAm/Other/Black/Hisp   -9.485e-03  1.864e-02  4.479e+03  -0.509   0.6109    
# prior_gpa                     4.525e-01  1.135e-02  3.854e+03  39.873   <2e-16 ***

class(lc_concurrent_effort_mod) <- "lmerMod"
stargazer(lc_concurrent_effort_mod,
          star.cutoffs = c(.05, .01, .001),
          notes        = "1 star p<.05; 2 stars p<.01; 3 stars p<.001",
          notes.label  = "",
          notes.append = FALSE,
          single.row=TRUE,
          header=FALSE,
          title = "Combined LC score predicts self-reported effort")

```


## H2

```{r}

lc_delta_effort_mod <- lmer(effort_ftl ~ comb_lc_score_ftl + comb_lc_score_first +
               gender + race + prior_gpa +
               ( 1 | class_id ) + ( 1 | teacher_id ) +
               ( 1 | team_id ),
             data = d_ftl)
summary(lc_delta_effort_mod)
#                                Estimate Std. Error         df t value Pr(>|t|)    
# (Intercept)                    -0.08095    0.14158  783.81132  -0.572   0.5677    
# comb_lc_score_ftl               0.22399    0.02033 2392.17172  11.017   <2e-16 ***
# comb_lc_score_first             0.02829    0.02032 2263.63302   1.392   0.1641    
# genderMale, Non-Binary/Other    0.00430    0.03389 2405.97170   0.127   0.8991    
# gendermasked                    0.02199    0.09030   10.42176   0.243   0.8124    
# raceNatAm/Other/Black/Hisp     -0.08165    0.03777 1889.10439  -2.162   0.0308 *  
# prior_gpa                      -0.01359    0.02330 1191.70503  -0.583   0.5599  

class(lc_delta_effort_mod) <- "lmerMod"
stargazer(lc_delta_effort_mod,
          star.cutoffs = c(.05, .01, .001),
          notes        = "1 star p<.05; 2 stars p<.01; 3 stars p<.001",
          notes.label  = "",
          notes.append = FALSE,
          single.row=TRUE,
          header=FALSE,
          title = "Cross-observation changes in students’ LCs predict corresponding changes in effort")



```

## H3 

```{r}
lc_classmates_mod <- lmer(comb_lc_score ~ comb_lc_score_classmates + gender + race + prior_gpa +
               ( 1 | student_id ) + ( 1 | team_id ),
             data = d)
summary(lc_classmates_mod)
class(lc_classmates_mod) <- "lmerMod"
stargazer(lc_classmates_mod,
          star.cutoffs = c(.05, .01, .001),
          notes        = "1 star p<.05; 2 stars p<.01; 3 stars p<.001",
          notes.label  = "",
          notes.append = FALSE,
          single.row=TRUE,
          header=FALSE,
          title = "Students’ personal perceptions of LCs are correlated with the LCs of their classmates")


```